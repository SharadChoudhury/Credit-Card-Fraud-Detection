Transfer csv file to emr:
scp -i my-pair1.pem Credit-Card-Fraud-Detection/card_transactions.csv hadoop@ec2-54-167-48-51.compute-1.amazonaws.com:/home/hadoop

Install Happybase:
sudo su
sudo yum update
yum install gcc
sudo yum install python3-devel
pip install happybase
pip install pandas

pip install kafka-python

Run below as root user
check if thrift server running:
jps
If not running:
hbase thrift start


Create a HBase table: 
hbase shell
create 'card_transactions', 'cf1'


Run the copy_to_hbase.py 

---------------------------------------------------------------------------

To enable sqoop connection to RDS
sudo su
wget https://de-mysql-connector.s3.amazonaws.com/mysql-connector-java-8.0.25.tar.gz
tar -xvf mysql-connector-java-8.0.25.tar.gz
cd mysql-connector-java-8.0.25/
sudo cp mysql-connector-java-8.0.25.jar /usr/lib/sqoop/lib/



Use sqoop to import table from RDS to Hive:

sqoop import \
--connect jdbc:mysql://upgradawsrds1.cyaielc9bmnf.us-east-1.rds.amazonaws.com/cred_financials_data \
--table card_member \
--username upgraduser --password upgraduser \
--hive-import \
--hive-table card_member \
--create-hive-table \
--fields-terminated-by ',' \
-m 1


sqoop import \
--connect jdbc:mysql://upgradawsrds1.cyaielc9bmnf.us-east-1.rds.amazonaws.com/cred_financials_data \
--table member_score \
--username upgraduser --password upgraduser \
--hive-import \
--hive-table member_score \
--create-hive-table \
--fields-terminated-by ',' \
-m 1


----------------------------------------------------------------------------
Set HBase - Hive integration:

Run the following in hbase shell: 
Do this only if you are running the Hive commands using Hue otherwise not required
grant 'hue' ,'RWXCA' (ignore even if it throws any errors)

-- Note: Before creating any table, make sure you run this command in Hive cli only (optional)
Add jar /usr/lib/hive-hcatalog/share/hcatalog/hive-hcatalog-core-2.3.6-amzn-2.jar ;


copy csv file to hadoop:
hadoop fs -put /home/hadoop/card_transactions.csv /user/hadoop/


Run the Hive_commands.sql file in Hue
To drop table in Hive: 
drop table <name>;


now check if records are written to the hbase table:
list : see if table appears in list output
scan 'hbase_lookup_table'


To drop table:
disable 'table'
drop 'table'


Records till 11-02-2018 in csv 
Records from 12-02-2018 in kafka
------------------------------------------------------------------------------
# write the transactions to card_transcations table in HBase
# def write_to_hbase(batch_df, batch_id):
#     batch_df \
#     .foreach(lambda row: hbase_instance.write_data(
#         f"{row.card_id}_{row.amount}_{row.transaction_dt}".encode(),
#         {   b'cf1:member_id': row.member_id.encode(), 
#             b'cf1:postcode': row.postcode.encode(),
#             b'cf1:pos_id': row.pos_id.encode(), 
#             b'cf1:status': row.status.encode()
#         },
#         'card_transactions'
#     ))

# Write to Console
# query = status_df \
#     .writeStream \
#     .foreachBatch(write_to_hbase) \
#     .outputMode("append") \
#     .format("console") \
#     .option("truncate", "false") \
#     .start() 


Actual execution:
unzip python.zip
cd python/src
zip src.zip __init__.py rules/* db/*

export SPARK_KAFKA_VERSION=0.10
spark-submit --py-files src.zip --files uszipsv.csv --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.5 driver.py
spark-submit --py-files copy_to_hbase.py read_new_trans.py

---------------------------------------------------------------------------------------------------------------------
