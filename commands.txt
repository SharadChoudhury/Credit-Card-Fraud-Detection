Transfer csv file to emr:
scp -i my-pair1.pem Credit-Card-Fraud-Detection/card_transactions.csv hadoop@ec2-34-226-138-78.compute-1.amazonaws.com:/home/hadoop

Install Happybase:
sudo -i
sudo yum update
yum install gcc
sudo yum install python3-devel
pip install happybase
pip install kafka-python

Run below as root user
check if thrift server running:
jps
If not running:
hbase thrift start


Create a HBase table: 
hbase shell
create 'card_transactions', 'cf1'


Run the copy_to_hbase.py 

---------------------------------------------------------------------------

To enable sqoop connection to RDS
sudo -i 
wget https://de-mysql-connector.s3.amazonaws.com/mysql-connector-java-8.0.25.tar.gz
tar -xvf mysql-connector-java-8.0.25.tar.gz
cd mysql-connector-java-8.0.25/
sudo cp mysql-connector-java-8.0.25.jar /usr/lib/sqoop/lib/



Use sqoop to import table from RDS to Hive:

sqoop import \
--connect jdbc:mysql://upgradawsrds1.cyaielc9bmnf.us-east-1.rds.amazonaws.com/cred_financials_data \
--table card_member \
--username upgraduser --password upgraduser \
--hive-import \
--hive-table card_member \
--create-hive-table \
--fields-terminated-by ',' \
-m 1


sqoop import \
--connect jdbc:mysql://upgradawsrds1.cyaielc9bmnf.us-east-1.rds.amazonaws.com/cred_financials_data \
--table member_score \
--username upgraduser --password upgraduser \
--hive-import \
--hive-table member_score \
--create-hive-table \
--fields-terminated-by ',' \
-m 1


----------------------------------------------------------------------------
Set HBase - Hive integration:
Run the following in hbase shell:
grant 'hue' ,'RWXCA' (ignore even if it throws any errors)


-- Note: Before creating any table, make sure you run this command in Hive cli only
Add jar /usr/lib/hive-hcatalog/share/hcatalog/hive-hcatalog-core-2.3.6-amzn-2.jar;


copy csv file to hadoop:
hadoop fs -put card_transactions.csv /user/hadoop/


Run the Hive_commands.sql file in Hue


now check if records are written to the hbase table:
list : see if table appears in list output
scan 'hbase_lookup_table'


To drop table:
disable 'table'
drop 'table'
------------------------------------------------------------------------------

export SPARK_KAFKA_VERSION=0.10

spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.5 ingest_from_kafka.py



