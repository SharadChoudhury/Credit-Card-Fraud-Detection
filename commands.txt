Transfer csv file to emr:
scp -i my-pair1.pem Credit-Card-Fraud-Detection/card_transactions.csv hadoop@ec2-54-82-55-141.compute-1.amazonaws.com:/home/hadoop

Install Happybase:
sudo su
sudo yum update
yum install gcc
sudo yum install python3-devel
pip install happybase
pip install pandas

pip install kafka-python

Run below as root user
check if thrift server running:
jps
If not running:
hbase thrift start


Create a HBase table: 
hbase shell
create 'card_transactions', 'cf1'

Run the copy_to_hbase.py 

---------------------------------------------------------------------------

To enable sqoop connection to RDS
sudo su
wget https://de-mysql-connector.s3.amazonaws.com/mysql-connector-java-8.0.25.tar.gz
tar -xvf mysql-connector-java-8.0.25.tar.gz
cd mysql-connector-java-8.0.25/
sudo cp mysql-connector-java-8.0.25.jar /usr/lib/sqoop/lib/



Use sqoop to import table from RDS to Hive:

sqoop import \
--connect jdbc:mysql://upgradawsrds1.cyaielc9bmnf.us-east-1.rds.amazonaws.com/cred_financials_data \
--table card_member \
--username upgraduser --password upgraduser \
--hive-import \
--hive-table card_member \
--create-hive-table \
--fields-terminated-by ',' \
-m 1


sqoop import \
--connect jdbc:mysql://upgradawsrds1.cyaielc9bmnf.us-east-1.rds.amazonaws.com/cred_financials_data \
--table member_score \
--username upgraduser --password upgraduser \
--hive-import \
--hive-table member_score \
--create-hive-table \
--fields-terminated-by ',' \
-m 1


----------------------------------------------------------------------------
Set HBase - Hive integration:

Run the following in hbase shell: 
Do this only if you are running the Hive commands using Hue otherwise not required
grant 'hue' ,'RWXCA' (ignore even if it throws any errors)

-- Note: Before creating any table, make sure you run this command in Hive cli only (optional)
Add jar /usr/lib/hive-hcatalog/share/hcatalog/hive-hcatalog-core-2.3.6-amzn-2.jar ;


copy csv file to hadoop:
hadoop fs -put /home/hadoop/card_transactions.csv /user/hadoop/


Run the Hive_commands.sql file in Hue
To drop table in Hive: 
drop table <name>;


now check if records are written to the hbase table:
list : see if table appears in list output
scan 'hbase_lookup_table'


To drop table:
disable 'table'
drop 'table'


Records till 11-02-2018 in csv 
Records from 12-02-2018 in kafka
------------------------------------------------------------------------------

1000 rows per batch :
.option("maxOffsetsPerTrigger", 1000) \

# Write to CSV
query2 = status_df \
    .writeStream \
    .format("csv") \
    .option("path", "/user/hadoop/new_trans") \
    .option("header", True) \
    .option("checkpointLocation", "/user/hadoop/checkpoint") \
    .outputMode("append") \
    .start()


Actual execution:
unzip python.zip
cd python/src
zip src.zip __init__.py rules/* db/* 

export SPARK_KAFKA_VERSION=0.10
spark-submit --py-files src.zip --files uszipsv.csv --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.5 driver.py


---------------------------------------------------------------------------------------------------------------------

Shortcut EMR startup:

scp -i my-pair1.pem Credit-Card-Fraud-Detection/extras/{genuine_trans.csv,copy_hbase_lookup.py} hadoop@ec2-54-82-55-141.compute-1.amazonaws.com:/home/hadoop